{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> &uarr;   Ensure Kernel is set to  &uarr;  </div><br><div style=\"text-align: right\"> conda_amazonei_tensorflow2_p36  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Estimator Bring your own Script\n",
    "\n",
    "In this notebook we will go through and run a tensorflow model to classify the junctions as priority, signal and roundabout as seen in data prep.\n",
    "\n",
    "The outline of this notebook is \n",
    "\n",
    "1. To prepare a training script (provided).\n",
    "\n",
    "2. Use the AWS provided Tensorflow container and provide our script to it.\n",
    "\n",
    "3. Run training.\n",
    "\n",
    "4. Deploy model to end point.\n",
    "\n",
    "5. Test using an image in couple of possible ways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade Sagemaker so we can access the latest containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'sagemaker>=2.48'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also upgrade out version of Tensorflow to v2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that our environment is using Tensorflow 2.4.1 otherwise we will need to restart the notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensorflow version {tf.__version__}\")\n",
    "\n",
    "if tf.__version__ != \"2.4.1\":\n",
    "    print(\"This notebook kernel needs to be restarted!!!!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing some libraries that we will be using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = True\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = \"[YOUR ROLE]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to make sure we are using the latest version of SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input params for model training \n",
    "\n",
    "In the cell below, replace **\"your-unique-bucket-name\"** with the name of bucket you created in the data-prep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"your-unique-bucket-name\"\n",
    "\n",
    "train_instance_type='ml.p3.2xlarge'      # The type of EC2 instance which will be used for training. \n",
    "deploy_instance_type='ml.m5.4xlarge'     # The type of EC2 instance which will be used for deployment\n",
    "\n",
    "training_data_uri=\"s3://{}\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator\n",
    "\n",
    "Starting with TensorFlow version 1.11, you can use Amazon **SageMaker prebuilt TensorFlow containers**: Simply provide (1) Python training script, (2) specify hyperparameters, and (3) indicate your training hardware configuration. SageMaker does the rest, including spinning up a training cluster and **tearing down the cluster when training ends**. \n",
    "\n",
    "These containers can be extended by starting with the image provided by AWS and the add additional installs in dockerfile or you can use requirements.txt in source_dir to install additional libraries.\n",
    "\n",
    "We setup the Tensorflow estimator job a job name, an entry point (which is our script **tfModelCode.py**), role, Tensorflow framework version, python version, instance count and type. <br>\n",
    "Then we call the estimators fit method with the URI of the training dataset to kick off the training job.<br>\n",
    "\n",
    "**Distributed Training**\n",
    "\n",
    "Before using distributed training in a cluster, we should first scale up on a single machine with more GPUs. Communication between multiple GPUs on a single machine is faster than communicating across a network between multiple machines. \n",
    "\n",
    "There are two options for distributed training.\n",
    "\n",
    "*Option 1: TensorFlow’s native parameter server* <br>\n",
    "\n",
    "Here, each instance in the cluster runs one parameter server process and one worker process. Each parameter server communicates with all workers (“all-to-all”). The implementation of parameter servers is asynchronous: each worker computes gradients and submits gradient updates to the parameter servers independently, without waiting for the other workers’ updates. Workers that fall behind might submit stale gradients, which can negatively affect training convergence. Generally, this can be managed by reducing the learning rate. On the plus side, because there is no waiting for other workers, asynchronous updates can result in faster training.\n",
    "\n",
    "distributions parameter will have to be defined in the Tensorflow Estimator.<br> \n",
    "`distributions = {'parameter_server': {'enabled': True}}`\n",
    "\n",
    "*Option 2: Horovod* \n",
    "\n",
    "Horovod is an open source framework for distributed deep learning. It is available for use with TensorFlow and several other deep learning frameworks. In Horovod updates are synchronous. After all processes have completed their calculations for the current batch, gradients calculated by each process circulate around the ring until every process has a complete set of gradients for the batch from all processes. At that time, each process updates its local model weights, so every process has the same model weights before starting work on the next batch.\n",
    "\n",
    "`distributions = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\"}}`\n",
    "\n",
    "if you use multi-GPU instance such as p3.8xlarge (4 GPU), you need to define the distributions as below.\n",
    "\n",
    "`distributions = {\"mpi\": {\"enabled\": True, \"processes_per_host\": 4}}`\n",
    "\n",
    "\n",
    "For more details on horovod implementation, please refer to:\n",
    "https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/\n",
    "\n",
    "\n",
    "**Note**: \n",
    "\n",
    "- In this example, the training took 25 min with 23 min training time one instance of 'ml.m5.12xlarge' (CPU)\n",
    "- It took the similar time with 2 instances of 'ml.m5.12xlarge' with distributed training enabled since data set is small (CPU)\n",
    "- It took total 11.5 min with 9 min training time with one instance of 'ml.p3.2xlarge' (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = { \"epochs\": 1, \"learning_rate\": 0.002}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_tf = TensorFlow(\n",
    "  base_job_name='tensorflow-traffic-class',\n",
    "  entry_point=\"tfModelCode.py\",             # Your entry script\n",
    "  role=role,\n",
    "  framework_version=\"2.4.1\",               # TensorFlow's version\n",
    "  py_version=\"py37\",\n",
    "  instance_count=1,  # \"The number of instances to use\"\n",
    "  distribution={ \"parameter_server\": { \"enabled\": False } },\n",
    "  hyperparameters = hyperparameters,\n",
    "  instance_type=train_instance_type,\n",
    ")\n",
    "\n",
    "print(\"Training ...\")\n",
    "estimator_tf.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:** <br>\n",
    "If at this point your kernel disconnects from the server (you can tell because the kernel in the top right hand corner will say **No Kernel**, you can reattach to the training job (so you dont to start the training job again.<br>Follow the steps below\n",
    "1. Scoll your notebook to the top and set the kernel to the recommended kernel specified in the top right hand corner of the notebook\n",
    "2. Go to your SageMaker console, Go to Training Jobs and copy the name of the training job you were disconnected from\n",
    "3. Scoll to the bottom of this notebook, paste your training job name to replace the **your-training-job-name** in the cell\n",
    "4. Run the edited cell\n",
    "5. Return to this cell and continue executing the rest of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a model\n",
    "Once trained, deploying a model is a simple call. <br>\n",
    "We specify two prarameters<br>\n",
    "    **instance_type** - the type of the instance will be used to do inference<br>\n",
    "    **initial_instance_count** - the initial number of instances that will be provisioned to do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deployed=estimator_tf.deploy(instance_type=deploy_instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the estimator has been deployed to an endpoint, lets find out the endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator_deployed.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do predictions against this endpoint, we are going to use **Predictor**. We provide it the endpoint name, the SageMaker session and the serializer (in our case a JSONSerializer)\n",
    "Serializers implement methods for serializing data for an inference endpoint<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "endpoint_name = estimator_deployed.endpoint_name   # you can also paste your endpoint name here from the cell above\n",
    "\n",
    "predictor=Predictor(endpoint_name=endpoint_name,\n",
    "                    sagemaker_session=sagemaker_session, \n",
    "                    serializer=JSONSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we install some convenience libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take one of our test images and apply some preprocessing to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='../data/test/Roundabout/R2.png'\n",
    "img = tf.keras.preprocessing.image.load_img(file, target_size=[250, 250])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = tf.keras.applications.efficientnet.preprocess_input(\n",
    "    x[tf.newaxis,...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send that processed data to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the prediction has sent back a confidence score for each class. The second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using boto3 sagemaker_runtime client\n",
    "\n",
    "So what if we want to make predictions against this endpoint outside of this notebook?<br>\n",
    "We then leverage the boto3 library. <br>\n",
    "**NOTE** Replace **'your-endpoint-name'** with your endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client=boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps({'instances':x.tolist()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the JSON response. Again the second value in the list corresponds to the class label \"Roundabout\" which has the highest confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response['Body'].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a S3 URI to the model artifacts package generated from the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator_tf.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us make sure we are in the correct starting folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/SageMaker/sageMakerWorkshop/tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we copy and unpack the model artifacts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_data} ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./export/model.tar.gz -C ./export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move the unpacked model artifacts folder to the 1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv tf000000001/1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r code/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now package up the code and 1 folder to create a new model.tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -czvf model.tar.gz code 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the new model.tar.gz to your S3 bucket and setup our Tensorflow Serving Container https://github.com/aws/sagemaker-tensorflow-serving-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sm_role=sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# See the following document for more on SageMaker Roles:\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\n",
    "role = sm_role\n",
    "\n",
    "# Will be using the bucket variable defined at beginning of this notebook\n",
    "\n",
    "prefix = 'tf_model'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "model_data = sagemaker_session.upload_data('model.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model'))\n",
    "                                           \n",
    "tensorflow_serving_model = TensorFlowModel(model_data=model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='2.4.1',\n",
    "                                 sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify the output folder and run the transformer method to start the batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "tensorflow_serving_transformer = tensorflow_serving_model.transformer(\n",
    "                                     instance_count=2,\n",
    "                                     instance_type='ml.m5.12xlarge',\n",
    "                                     max_concurrent_transforms=64,\n",
    "                                     max_payload=1,\n",
    "                                     output_path=output_path)\n",
    "\n",
    "input_path = f's3://{bucket}/test'\n",
    "tensorflow_serving_transformer.transform(input_path, content_type='application/x-image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this batch job will be in the following S3 URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the output file from the batch job. Each file is a prediction that corresponds to the input image file name. You can have a look at the predictions inside the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {output_path} --recursive | grep -v \".ipy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Attach to a training job that has been left to run \n",
    "\n",
    "If your kernel becomes disconnected and your training has already started, you can reattach to the training job.<br>\n",
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook<br>\n",
    "Then look up the training job name and replace the **your-training-job-name** and then run the cell below. <br>\n",
    "Once the training job is finished, you can continue the cells after the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"your-unique-bucket-name\"\n",
    "\n",
    "training_job_name = 'your-training-job-name'\n",
    "\n",
    "estimator_tf = TensorFlow.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways ###\n",
    "\n",
    "Some benefits of using SageMaker for training and inference are:\n",
    "\n",
    "- We can automatically provision specialist computing resources (e.g. high-performance, or GPU-accelerated instances) for **only** the duration of the training job: Getting good performance in training, without leaving resources sitting around under-utilized\n",
    "- The history of training jobs (including parameters, metrics, outputs, etc.) is automatically tracked - unlike local notebook experiments where the user needs to keep notes on what worked and what didn't\n",
    "- Our trained model can be deployed to a secure, production-ready web endpoint with just one SDK call: No container or web application packaging required, unless we want to customize the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
